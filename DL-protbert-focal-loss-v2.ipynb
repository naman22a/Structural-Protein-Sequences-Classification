{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd9ca6a-70cc-4927-885a-11b7a062cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b94da3e-043d-4657-8abb-df0d13329a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71573</th>\n",
       "      <td>XEWEALEKKLAALESKXQALEKKLEALEHGX</td>\n",
       "      <td>DE NOVO PROTEIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257713</th>\n",
       "      <td>GPGSMSIPTLNPTVALVAIDLQNGIVVLPMVPQSGGDVVAKTAELA...</td>\n",
       "      <td>UNKNOWN FUNCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169619</th>\n",
       "      <td>ALLSFERKYRVPGGTLVGGNLFDFWVGPFYVGFFGVATFFFAALGI...</td>\n",
       "      <td>PHOTOSYNTHESIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13661</th>\n",
       "      <td>MGHHHHHHSGEDEQQEQTIAEDLVVTKYKMGGDIANRVLRSLVEAS...</td>\n",
       "      <td>TRANSCRIPTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60685</th>\n",
       "      <td>XFMAFWEXLX</td>\n",
       "      <td>CELL CYCLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sequence    classification\n",
       "71573                     XEWEALEKKLAALESKXQALEKKLEALEHGX   DE NOVO PROTEIN\n",
       "257713  GPGSMSIPTLNPTVALVAIDLQNGIVVLPMVPQSGGDVVAKTAELA...  UNKNOWN FUNCTION\n",
       "169619  ALLSFERKYRVPGGTLVGGNLFDFWVGPFYVGFFGVATFFFAALGI...    PHOTOSYNTHESIS\n",
       "13661   MGHHHHHHSGEDEQQEQTIAEDLVVTKYKMGGDIANRVLRSLVEAS...     TRANSCRIPTION\n",
       "60685                                          XFMAFWEXLX        CELL CYCLE"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/protein_data.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_sampled, _ = train_test_split(\n",
    "    data,\n",
    "    stratify=data['classification'],\n",
    "    # train_size=10_000,\n",
    "    train_size=20_000,\n",
    "    random_state=42\n",
    ")\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3f4ad9-c17c-489f-a19d-cd9c8124f16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d33bcf-4ed1-4d7b-a3a2-ba69c7c28c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification\n",
       "DE NOVO PROTEIN                            625\n",
       "UNKNOWN FUNCTION                           625\n",
       "SIGNALING PROTEIN                          625\n",
       "IMMUNE SYSTEM                              625\n",
       "STRUCTURAL GENOMICS, UNKNOWN FUNCTION      625\n",
       "OXIDOREDUCTASE/OXIDOREDUCTASE INHIBITOR    625\n",
       "CHAPERONE                                  625\n",
       "HYDROLASE                                  625\n",
       "LIGASE                                     625\n",
       "TRANSFERASE                                625\n",
       "GENE REGULATION                            625\n",
       "TRANSFERASE/TRANSFERASE INHIBITOR          625\n",
       "RNA BINDING PROTEIN                        625\n",
       "STRUCTURAL PROTEIN                         625\n",
       "PROTEIN TRANSPORT                          625\n",
       "CELL ADHESION                              625\n",
       "OXIDOREDUCTASE                             625\n",
       "PROTEIN BINDING                            625\n",
       "TRANSPORT PROTEIN                          625\n",
       "TOXIN                                      625\n",
       "DNA BINDING PROTEIN                        625\n",
       "VIRAL PROTEIN                              625\n",
       "METAL BINDING PROTEIN                      625\n",
       "SUGAR BINDING PROTEIN                      625\n",
       "MEMBRANE PROTEIN                           625\n",
       "ISOMERASE                                  625\n",
       "LYASE                                      625\n",
       "ELECTRON TRANSPORT                         625\n",
       "CELL CYCLE                                 625\n",
       "TRANSCRIPTION                              625\n",
       "PHOTOSYNTHESIS                             625\n",
       "HYDROLASE/HYDROLASE INHIBITOR              625\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled['classification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1db75d2-1cb6-48c3-908b-633f26172e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sampled['sequence']\n",
    "y = df_sampled['classification']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "n_classes = len(class_names)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db6e9cc-cee9-4a51-ba16-40c19923e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length: 241.07\n"
     ]
    }
   ],
   "source": [
    "avg_char_len = X_train.str.len().mean()\n",
    "print(f\"Average character length: {avg_char_len:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afae0115-e82b-400d-8397-820a1573c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "\n",
    "def preprocess_sequence(sequence):\n",
    "    return ' '.join(list(sequence.strip()))\n",
    "\n",
    "def tokenize_sequences(sequences, max_length=250):\n",
    "    sequences = [preprocess_sequence(seq) for seq in sequences]\n",
    "    return tokenizer(\n",
    "        sequences,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4971c02-b587-435d-945a-ae7c3c10320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, max_length=250):\n",
    "        self.labels = labels\n",
    "        tokenized = tokenize_sequences(sequences, max_length=max_length)\n",
    "        self.input_ids = tokenized[\"input_ids\"]\n",
    "        self.attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = ProteinDataset(X_train.tolist(), y_train.tolist())\n",
    "val_dataset = ProteinDataset(X_val.tolist(), y_val.tolist())\n",
    "test_dataset = ProteinDataset(X_test.tolist(), y_test.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf68f09d-5abf-411e-b847-5b6752b8161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ae0be9-356b-4d53-a0b4-756aa0aae565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0, path=\"checkpoint.pt\", verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement.\n",
    "            delta (float): Minimum change in monitored metric to qualify as improvement.\n",
    "            path (str): File path to save the best model.\n",
    "            verbose (bool): Print messages when improvement happens.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5ed36b-4e06-4c3e-a4f0-970bd0446ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = self.ce(logits, targets)\n",
    "        pt = torch.exp(-ce_loss)  # probability of the true class\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0308b325-0697-4559-8cab-9c26f53bdbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43f103c-caee-4003-85f5-e8efcd82832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# suppose y_train contains your training labels (numpy array)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c383ba1-6921-4b45-9977-eb53c2704dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 3.3566 | Train Acc: 0.0326 | Val Loss: 3.3430 | Val Acc: 0.0194 | Time: 80.45s\n",
      "Validation loss decreased (inf → 3.343016). Saving model...\n",
      "Epoch 2/20 | Train Loss: 3.2306 | Train Acc: 0.0533 | Val Loss: 3.0883 | Val Acc: 0.0469 | Time: 80.00s\n",
      "Validation loss decreased (3.343016 → 3.088318). Saving model...\n",
      "Epoch 3/20 | Train Loss: 3.0052 | Train Acc: 0.0615 | Val Loss: 2.9323 | Val Acc: 0.0706 | Time: 80.01s\n",
      "Validation loss decreased (3.088318 → 2.932322). Saving model...\n",
      "Epoch 4/20 | Train Loss: 2.8942 | Train Acc: 0.0625 | Val Loss: 2.8949 | Val Acc: 0.0544 | Time: 80.06s\n",
      "Validation loss decreased (2.932322 → 2.894910). Saving model...\n",
      "Epoch 5/20 | Train Loss: 2.8312 | Train Acc: 0.0619 | Val Loss: 2.8435 | Val Acc: 0.0725 | Time: 80.06s\n",
      "Validation loss decreased (2.894910 → 2.843506). Saving model...\n",
      "Epoch 6/20 | Train Loss: 2.7738 | Train Acc: 0.0652 | Val Loss: 2.8246 | Val Acc: 0.0737 | Time: 80.00s\n",
      "Validation loss decreased (2.843506 → 2.824582). Saving model...\n",
      "Epoch 7/20 | Train Loss: 2.7425 | Train Acc: 0.0599 | Val Loss: 2.7937 | Val Acc: 0.0725 | Time: 80.12s\n",
      "Validation loss decreased (2.824582 → 2.793744). Saving model...\n",
      "Epoch 8/20 | Train Loss: 2.7076 | Train Acc: 0.0669 | Val Loss: 2.7857 | Val Acc: 0.0594 | Time: 80.14s\n",
      "Validation loss decreased (2.793744 → 2.785720). Saving model...\n",
      "Epoch 9/20 | Train Loss: 2.6758 | Train Acc: 0.0699 | Val Loss: 2.7689 | Val Acc: 0.0644 | Time: 80.07s\n",
      "Validation loss decreased (2.785720 → 2.768935). Saving model...\n",
      "Epoch 10/20 | Train Loss: 2.6505 | Train Acc: 0.0682 | Val Loss: 2.7645 | Val Acc: 0.0762 | Time: 80.24s\n",
      "Validation loss decreased (2.768935 → 2.764493). Saving model...\n",
      "Epoch 11/20 | Train Loss: 2.6305 | Train Acc: 0.0711 | Val Loss: 2.7519 | Val Acc: 0.0825 | Time: 80.46s\n",
      "Validation loss decreased (2.764493 → 2.751906). Saving model...\n",
      "Epoch 12/20 | Train Loss: 2.6118 | Train Acc: 0.0761 | Val Loss: 2.7465 | Val Acc: 0.0781 | Time: 80.49s\n",
      "Validation loss decreased (2.751906 → 2.746490). Saving model...\n",
      "Epoch 13/20 | Train Loss: 2.5961 | Train Acc: 0.0759 | Val Loss: 2.7355 | Val Acc: 0.0950 | Time: 80.48s\n",
      "Validation loss decreased (2.746490 → 2.735470). Saving model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     66\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     69\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    459\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 461\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    463\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    349\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    354\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    356\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    349\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    354\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    356\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ProteinClassifier(num_labels=n_classes).to(device)\n",
    "\n",
    "# Freeze BERT encoder\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "for name, param in model.bert.named_parameters():\n",
    "    if \"encoder.layer.28\" in name or \"encoder.layer.29\" in name or \"encoder.layer.30\" in name or \"encoder.layer.31\" in name:\n",
    "        param.requires_grad = True   # unfreeze last 4 layers\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "# optimizer = AdamW([\n",
    "#     {\"params\": model.bert.parameters(), \"lr\": 2e-5},        # pretrained encoder (small LR)\n",
    "#     {\"params\": model.classifier.parameters(), \"lr\": 1e-4},  # classifier head (larger LR)\n",
    "# ], weight_decay=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=1.0)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "# History dictionary\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'epoch_time': []\n",
    "}\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path=\"./models/probert_focal_loss_v2.pt\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_train_preds.extend(preds.cpu().numpy())\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_acc = accuracy_score(all_train_labels, all_train_preds)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "          f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['epoch_time'].append(elapsed)\n",
    "\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666f2f7-8579-4763-a27a-259250f90e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "plt.legend()\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d5f90-64a5-4838-9e3a-0e373be5b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history['val_acc'], label='Val Accuracy', marker='o')\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b8e5e-244d-47ac-a4b2-c2f7ad90b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adef9c-8ec3-49fa-9b71-f1e2217bd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision = precision_score(test_labels, test_preds, average=\"weighted\")\n",
    "recall = recall_score(test_labels, test_preds, average=\"weighted\")\n",
    "f1 = f1_score(test_labels, test_preds, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0d808-c4ec-4e30-983d-3104932c0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae3b01-8356-405f-a2fd-fec5fe178743",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4d150-60e2-41ff-81da-406577f806c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
